{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Full Setup, Imports, and Helper Function Definitions\n",
        "\n",
        "# --- Basic Imports and Setup ---\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Scikit-learn and MLxtend Imports ---\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
        "\n",
        "# --- Hugging Face Transformers Imports ---\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    BertPreTrainedModel,\n",
        "    BertModel\n",
        ")\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- KEY SETTINGS ---\n",
        "GDRIVE_PATH = '/content/drive/MyDrive/eecsi_revise/'\n",
        "SEED = 42\n",
        "TEXT_COLUMN = 'cleaned_text'\n",
        "\n",
        "# --- Custom Class Definitions (Required to load models) ---\n",
        "class IndoBERT_MTL(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.aspect_classifier = nn.Linear(config.hidden_size, config.num_aspect_labels)\n",
        "        self.sentiment_classifier = nn.Linear(config.hidden_size, config.num_sentiment_labels)\n",
        "        self.init_weights()\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, aspect_labels=None, sentiment_labels=None, return_dict=None, **kwargs):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=return_dict)\n",
        "        pooled_output = outputs[1]; pooled_output = self.dropout(pooled_output)\n",
        "        aspect_logits = self.aspect_classifier(pooled_output); sentiment_logits = self.sentiment_classifier(pooled_output)\n",
        "        # Handle prediction case where no labels are passed\n",
        "        if aspect_labels is None:\n",
        "            return SequenceClassifierOutput(loss=None, logits=(aspect_logits, sentiment_logits), hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
        "        # Handle training case\n",
        "        else:\n",
        "            total_loss = 0; loss_fct = nn.CrossEntropyLoss()\n",
        "            total_loss += loss_fct(aspect_logits.view(-1, self.config.num_aspect_labels), aspect_labels.view(-1))\n",
        "            total_loss += loss_fct(sentiment_logits.view(-1, self.config.num_sentiment_labels), sentiment_labels.view(-1))\n",
        "            return SequenceClassifierOutput(loss=total_loss, logits=(aspect_logits, sentiment_logits), hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
        "\n",
        "class PredictionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings): self.encodings = encodings\n",
        "    def __getitem__(self, idx): return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self): return len(self.encodings.input_ids)\n",
        "\n",
        "# --- Helper Function to Find Best Checkpoint ---\n",
        "def find_best_checkpoint(fold_dir):\n",
        "    try:\n",
        "        if not os.path.exists(fold_dir): raise FileNotFoundError(f\"Directory not found: {fold_dir}\")\n",
        "        possible_checkpoints = [d for d in os.listdir(fold_dir) if d.startswith('checkpoint-') and os.path.isdir(os.path.join(fold_dir, d))]\n",
        "        if not possible_checkpoints: raise FileNotFoundError(f\"No checkpoint folders found inside {fold_dir}\")\n",
        "        latest_checkpoint_dir = sorted(possible_checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n",
        "        state_path = os.path.join(fold_dir, latest_checkpoint_dir, 'trainer_state.json')\n",
        "        with open(state_path, 'r') as f: state = json.load(f)\n",
        "        best_checkpoint_name = os.path.basename(state['best_model_checkpoint'])\n",
        "        best_checkpoint_path = os.path.join(fold_dir, best_checkpoint_name)\n",
        "        print(f\"  ✅ Best model confirmed: {best_checkpoint_name}\")\n",
        "        return best_checkpoint_path\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ ERROR: Could not determine best model for {fold_dir}. Reason: {e}. Skipping fold.\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ Setup complete. All libraries, classes, and helper functions are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gpPMj9gkpi",
        "outputId": "d7a7920c-d354-440b-d378-878ada973192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Setup complete. All libraries, classes, and helper functions are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Generate or Load All ASC Predictions\n",
        "\n",
        "asc_predictions_path = os.path.join(GDRIVE_PATH, 'all_asc_predictions.pkl')\n",
        "\n",
        "if os.path.exists(asc_predictions_path):\n",
        "    print(f\"--- Loading existing ASC predictions from file ---\")\n",
        "    with open(asc_predictions_path, 'rb') as f:\n",
        "        all_asc_predictions = pickle.load(f)\n",
        "    print(\"✅ ASC Predictions loaded successfully.\")\n",
        "else:\n",
        "    print(f\"--- Generating ASC predictions as file was not found. This may take a few minutes... ---\")\n",
        "\n",
        "    # --- 1. Load Data and Filter for Relevant Tweets ---\n",
        "    df = pd.read_csv(os.path.join(GDRIVE_PATH, 'final_golden_dataset_eecsi.csv'))\n",
        "    relevant_df = df[df['aspect'] != 'Irrelevant'].copy()\n",
        "    print(f\"Found {len(relevant_df)} relevant tweets for ASC analysis.\")\n",
        "\n",
        "    # --- 2. Setup (Labels, etc.) ---\n",
        "    sentiment_labels_list = sorted(relevant_df['sentiment'].unique())\n",
        "    s_label2id = {l: i for i, l in enumerate(sentiment_labels_list)}\n",
        "    s_id2label = {i: l for i, l in enumerate(sentiment_labels_list)}\n",
        "\n",
        "    # --- 3. Prediction Function for PIPELINE Models ---\n",
        "    def get_pipeline_asc_predictions(model_id, results_path):\n",
        "        print(f\"\\n--- Processing PIPELINE model: {os.path.basename(os.path.normpath(results_path))} ---\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model_preds = pd.Series([None] * len(relevant_df), index=relevant_df.index)\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "        for i, (train_index, test_index) in enumerate(skf.split(relevant_df, relevant_df['sentiment'])):\n",
        "            fold_num = i + 1; print(f\"  Processing Fold {fold_num}/5...\")\n",
        "            fold_dir = os.path.join(results_path, f'fold_{fold_num}')\n",
        "            if not os.path.exists(fold_dir): fold_dir = os.path.join(results_path, f'results_fold_{fold_num}')\n",
        "            best_checkpoint_path = find_best_checkpoint(fold_dir)\n",
        "            if not best_checkpoint_path: continue\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint_path)\n",
        "            trainer = Trainer(model=model)\n",
        "            test_data = relevant_df.iloc[test_index]\n",
        "            test_encodings = tokenizer(list(test_data[TEXT_COLUMN]), truncation=True, padding=True, max_length=128)\n",
        "            prediction_dataset = PredictionDataset(test_encodings)\n",
        "            predictions = trainer.predict(prediction_dataset)\n",
        "            predicted_labels_int = np.argmax(predictions.predictions, axis=1)\n",
        "            model_preds.iloc[test_index] = predicted_labels_int\n",
        "        return model_preds.map(s_id2label)\n",
        "\n",
        "    # --- 4. Prediction Function for MTL Models ---\n",
        "    def get_mtl_asc_predictions(model_id, results_path):\n",
        "        print(f\"\\n--- Processing MTL model: {os.path.basename(os.path.normpath(results_path))} ---\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model_preds = pd.Series([None] * len(relevant_df), index=relevant_df.index)\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "        for i, (train_index, test_index) in enumerate(skf.split(relevant_df, relevant_df['sentiment'])):\n",
        "            fold_num = i + 1; print(f\"  Processing Fold {fold_num}/5...\")\n",
        "            fold_dir = os.path.join(results_path, f'fold_{fold_num}')\n",
        "            if not os.path.exists(fold_dir): fold_dir = os.path.join(results_path, f'results_fold_{fold_num}')\n",
        "            best_checkpoint_path = find_best_checkpoint(fold_dir)\n",
        "            if not best_checkpoint_path: continue\n",
        "            model = IndoBERT_MTL.from_pretrained(best_checkpoint_path)\n",
        "            trainer = Trainer(model=model)\n",
        "            test_data = relevant_df.iloc[test_index]\n",
        "            test_encodings = tokenizer(list(test_data[TEXT_COLUMN]), truncation=True, padding=True, max_length=128)\n",
        "            prediction_dataset = PredictionDataset(test_encodings)\n",
        "            predictions = trainer.predict(prediction_dataset)\n",
        "            sentiment_logits = predictions.predictions[1] # Take the second output for sentiment\n",
        "            predicted_labels_int = np.argmax(sentiment_logits, axis=1)\n",
        "            model_preds.iloc[test_index] = predicted_labels_int\n",
        "        return model_preds.map(s_id2label)\n",
        "\n",
        "    # --- 5. Generate Predictions for ALL Key Models ---\n",
        "    all_asc_predictions = {\n",
        "        'y_true_sentiment': relevant_df['sentiment'],\n",
        "        'indobertweet_pipeline_asc': get_pipeline_asc_predictions(\"indolem/indobertweet-base-uncased\", os.path.join(GDRIVE_PATH, 'indobertweet_asc_results/')),\n",
        "        'indobert_pipeline_asc': get_pipeline_asc_predictions(\"indobenchmark/indobert-base-p1\", os.path.join(GDRIVE_PATH, 'indobert_asc_results/')),\n",
        "        'mtl_indobert_asc': get_mtl_asc_predictions(\"indobenchmark/indobert-base-p1\", os.path.join(GDRIVE_PATH, 'mtl_indobert_results/')),\n",
        "        'mtl_indobertweet_asc': get_mtl_asc_predictions(\"indolem/indobertweet-base-uncased\", os.path.join(GDRIVE_PATH, 'mtl_indobertweet_results/'))\n",
        "    }\n",
        "\n",
        "    with open(asc_predictions_path, 'wb') as f:\n",
        "        pickle.dump(all_asc_predictions, f)\n",
        "    print(f\"\\n✅ All ASC raw predictions have been generated and saved to: {asc_predictions_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "er-zsbXEgqGu",
        "outputId": "5f92a8a1-37c3-49bd-d5cc-59f7b91fd314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating ASC predictions as file was not found. This may take a few minutes... ---\n",
            "Found 2037 relevant tweets for ASC analysis.\n",
            "\n",
            "--- Processing PIPELINE model: indobertweet_asc_results ---\n",
            "  Processing Fold 1/5...\n",
            "  ✅ Best model confirmed: checkpoint-411\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 2/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 3/5...\n",
            "  ✅ Best model confirmed: checkpoint-274\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 4/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 5/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing PIPELINE model: indobert_asc_results ---\n",
            "  Processing Fold 1/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 2/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 3/5...\n",
            "  ✅ Best model confirmed: checkpoint-685\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 4/5...\n",
            "  ✅ Best model confirmed: checkpoint-548\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 5/5...\n",
            "  ✅ Best model confirmed: checkpoint-137\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing MTL model: mtl_indobert_results ---\n",
            "  Processing Fold 1/5...\n",
            "  ✅ Best model confirmed: checkpoint-510\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 2/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 3/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 4/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 5/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing MTL model: mtl_indobertweet_results ---\n",
            "  Processing Fold 1/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 2/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 3/5...\n",
            "  ✅ Best model confirmed: checkpoint-510\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 4/5...\n",
            "  ✅ Best model confirmed: checkpoint-510\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processing Fold 5/5...\n",
            "  ✅ Best model confirmed: checkpoint-408\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ All ASC raw predictions have been generated and saved to: /content/drive/MyDrive/eecsi_revise/all_asc_predictions.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Run McNemar's Test for Final Justification\n",
        "\n",
        "print(\"\\n--- McNemar's Test for Aspect Sentiment Classification (ASC) ---\")\n",
        "print(\"Goal: To justify the selection of the Pipeline architecture over the MTL architecture.\")\n",
        "print(\"Comparing: IndoBERTweet Pipeline (Champion) vs. IndoBERT MTL (Competitor) on RELEVANT data.\")\n",
        "\n",
        "# Load the comprehensive ASC predictions you just generated\n",
        "with open(asc_predictions_path, 'rb') as f:\n",
        "    all_asc_predictions = pickle.load(f)\n",
        "\n",
        "# --- CORRECTED MODEL SELECTION FOR COMPARISON ---\n",
        "# Prepare the data for the comparison you actually want to make\n",
        "y_true = np.array(all_asc_predictions['y_true_sentiment'])\n",
        "y_model1 = np.array(all_asc_predictions['indobertweet_pipeline_asc']) # Your Pipeline Champion\n",
        "y_model2 = np.array(all_asc_predictions['mtl_indobert_asc'])         # The MTL Champion Competitor\n",
        "\n",
        "# Ensure there are no missing values which can cause errors\n",
        "valid_indices = pd.Series(y_true).notna() & pd.Series(y_model1).notna() & pd.Series(y_model2).notna()\n",
        "y_true = y_true[valid_indices]\n",
        "y_model1 = y_model1[valid_indices]\n",
        "y_model2 = y_model2[valid_indices]\n",
        "\n",
        "\n",
        "# Create the contingency table\n",
        "tb = mcnemar_table(y_target=y_true, y_model1=y_model1, y_model2=y_model2)\n",
        "print(\"\\nContingency Table (Model 1 = IndoBERTweet Pipeline, Model 2 = IndoBERT MTL):\")\n",
        "print(tb)\n",
        "\n",
        "# Perform McNemar's test\n",
        "chi2, p = mcnemar(ary=tb, corrected=True)\n",
        "print(f\"\\nMcNemar's test results:\")\n",
        "print(f\"  Chi-squared statistic: {chi2:.4f}\")\n",
        "print(f\"  p-value: {p:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print(\"\\nConclusion: The performance difference between the Pipeline and MTL architectures for ASC is NOT statistically significant.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: The performance difference between the Pipeline and MTL architectures for ASC IS statistically significant.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtD0TG3Ogs_4",
        "outputId": "66faff8a-638d-4be1-9647-b06d4882b0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- McNemar's Test for Aspect Sentiment Classification (ASC) ---\n",
            "Goal: To justify the selection of the Pipeline architecture over the MTL architecture.\n",
            "Comparing: IndoBERTweet Pipeline (Champion) vs. IndoBERT MTL (Competitor) on RELEVANT data.\n",
            "\n",
            "Contingency Table (Model 1 = IndoBERTweet Pipeline, Model 2 = IndoBERT MTL):\n",
            "[[1672   34]\n",
            " [ 275   56]]\n",
            "\n",
            "McNemar's test results:\n",
            "  Chi-squared statistic: 186.4078\n",
            "  p-value: 0.0000\n",
            "\n",
            "Conclusion: The performance difference between the Pipeline and MTL architectures for ASC IS statistically significant.\n"
          ]
        }
      ]
    }
  ]
}